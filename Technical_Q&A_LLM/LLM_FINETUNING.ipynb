{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d160a60-48fa-4ad8-bf64-e6a55554b324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install evaluate\n",
    "!pip install nltk\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install sacrebleu\n",
    "!pip install codebleu\n",
    "!pip install tree-sitter-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc639a-a034-4723-9465-1013b7f2163c",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80328c6e-0980-4463-8579-7afedc397c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk\n",
    "from datasets import Dataset\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer,\n",
    "    DataCollatorForSeq2Seq, BitsAndBytesConfig, pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType,prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6594d-1992-4369-b22e-4bd6aa1e60c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare data for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4e1fc3-b65b-41f9-a956-d74fd508ddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 45000, Test size: 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b5ce5d5eaa4008ab70c7e05b18cd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load dataset (Ensure it has 'prompt' and 'response' columns)\n",
    "df = pd.read_json(\"fine_tuning_dataset.jsonl\",lines=True)  \n",
    "\n",
    "# Select 100K samples for fine-tuning\n",
    "df = df[['prompt', 'completion']].dropna().sample(n=10000, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Train-Test Split (90% Train, 10% Test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Prevents padding issues\n",
    "\n",
    "# Formatting function (ensures structured input-output)\n",
    "def format_data(example):\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(example[\"prompt\"], padding=\"max_length\", truncation=True, max_length=256)[\"input_ids\"],\n",
    "        \"attention_mask\": tokenizer(example[\"prompt\"], padding=\"max_length\", truncation=True, max_length=256)[\"attention_mask\"],\n",
    "        \"labels\": tokenizer(example[\"completion\"], padding=\"max_length\", truncation=True, max_length=256)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Apply tokenization & formatting\n",
    "tokenized_dataset = dataset.map(format_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "130b9174-a148-4056-925e-9af191de7767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02b71c7e1784d798e2dc8ca6a6e55e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb9cf566b0f4eacb83e3b4cfe4a543d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(format_data, remove_columns=[\"prompt\", \"completion\"])\n",
    "test_dataset = test_dataset.map(format_data, remove_columns=[\"prompt\", \"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249439d-2ffc-4645-8ac9-7bc5f1f542db",
   "metadata": {},
   "source": [
    "## Set model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0696cad7-6dbf-4b47-b562-269f58a322d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54036f85b2447d3bca324c593e97620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,935,296 || trainable%: 0.1243\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else {\"\": \"cpu\"}\n",
    "\n",
    "# ✅ Load 4-bit quantized model and explicitly assign device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# ✅ Move model to CUDA if it's not properly assigned\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.cuda.current_device())\n",
    "\n",
    "# Prepare model for fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Attach LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Ensure LoRA is correctly attached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42317e5-f95d-4f37-af82-ffb8434fb10d",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4b1f0-3fd6-43e4-a88a-8a5224d26f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Define data collator (Handles padding for efficiency)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codellama-7b-instruct-lora\",\n",
    "    per_device_train_batch_size=16,  # Keep batch small due to memory constraints\n",
    "    gradient_accumulation_steps=4,  # Increase accumulation to simulate a bigger batch\n",
    "    learning_rate=2e-4,  # Lower LR for stability\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,  # Save less frequently to reduce overhead\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,  # Evaluate less frequently to speed up training\n",
    "    logging_dir=\"./logs\",\n",
    "    optim=\"adamw_bnb_8bit\",  # Optimized for QLoRA\n",
    "    fp16 = True,  # BF16 instead of FP16 for efficiency\n",
    "    torch_compile=False,  # ✅ Disable for better speed\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,  # ✅ Speeds up training by reducing memory usage\n",
    ")\n",
    "# Initialize Trainer\n",
    "import torch\n",
    "if torch.cuda.get_device_capability(0)[0] >= 8:  # A100, H100, L40, etc.\n",
    "    model.enable_input_require_grads()\n",
    "    from accelerate import dispatch_model\n",
    "    model = dispatch_model(model,device_map=device_map)\n",
    "    print(\"✅ Flash Attention 2 Enabled!\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator)\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"codellama-7b-instruct-lora-finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f043ea6-e1a9-4d5e-ab00-866b5d2a9229",
   "metadata": {},
   "source": [
    "## Evaluating Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e441754d-2a83-4ef0-af4c-fb1f9ee94691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89109f7678e64425a0e40c3ee57d2c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # ✅ Frees up unused memory\n",
    "\n",
    "# Load Fine-Tuned Model\n",
    "MODEL_NAME = \"codellama-7b-instruct-lora/checkpoint-2109\"  # Update with actual model path\n",
    "device_map ={\"\": \"cpu\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,device_map=device_map)\n",
    "\n",
    "# Load Test Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a0232-c016-4e44-be89-3cb0e2d493cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"fine_tuning_dataset.jsonl\",lines=True)  \n",
    "\n",
    "# Select 100K samples for fine-tuning\n",
    "df = df[['prompt', 'completion']].dropna().sample(n=20, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "846ebe7e-9523-4927-8782-8b5efd48f615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuned CodeLlama Evaluation Results ---\n",
      "Perplexity (Lower is better): 9427028.416\n",
      "BLEU Score (Higher is better): 0.103\n",
      "Pass@3 (Higher is better): 0.000\n",
      "Execution Accuracy (Higher is better): 0.000\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(text):\n",
    "    \"\"\"Computes perplexity for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256).to(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss.item()\n",
    "    return math.exp(loss)\n",
    "\n",
    "# Compute Perplexity on test set\n",
    "perplexities = [calculate_perplexity(example[\"prompt\"]) for example in test_dataset]\n",
    "avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "\n",
    "# --------------------------\n",
    "# 4. BLEU Score Calculation\n",
    "# --------------------------\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def calculate_bleu(reference, prediction):\n",
    "    \"\"\"Computes BLEU score for a prediction against a reference.\"\"\"\n",
    "    if isinstance(reference, list):  # Convert tokenized text back to string\n",
    "        reference = \" \".join(reference)\n",
    "    if isinstance(prediction, list):\n",
    "        prediction = \" \".join(prediction)\n",
    "\n",
    "    return bleu_metric.compute(predictions=[prediction], references=[[reference]])[\"bleu\"]\n",
    "\n",
    "# --------------------------\n",
    "# 5. Pass@k (Execution-Based Metric)\n",
    "# --------------------------\n",
    "def pass_at_k(completions, reference_outputs, k=3):\n",
    "    \"\"\"Computes Pass@k metric.\"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(completions)):\n",
    "        if reference_outputs[i] in completions[i][:k]:  # If the correct answer is in the top-k predictions\n",
    "            correct += 1\n",
    "    return correct / len(completions)\n",
    "\n",
    "# --------------------------\n",
    "# 6. Functional Testing (Unit Test Execution)\n",
    "# --------------------------\n",
    "def evaluate_execution(generated_code, expected_output):\n",
    "    \"\"\"Executes generated code and checks if output matches expected.\"\"\"\n",
    "    try:\n",
    "        local_vars = {}\n",
    "        exec(generated_code, {}, local_vars)  # Execute in isolated namespace\n",
    "        return local_vars.get(\"output\") == expected_output  # Assumes function stores result in `output`\n",
    "    except Exception:\n",
    "        return False  # Fail on exceptions\n",
    "\n",
    "# --------------------------\n",
    "# 7. Run Evaluation\n",
    "# --------------------------\n",
    "generated_outputs = []\n",
    "bleu_scores = []\n",
    "execution_results = []\n",
    "\n",
    "for example in test_dataset:\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(example[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=256).to(\"cpu\")\n",
    "    \n",
    "    # Generate CodeLlama Output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100)  # ✅ Use max_new_tokens\n",
    "\n",
    "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)  # ✅ Convert tokens to text\n",
    "    generated_outputs.append(generated_code)\n",
    "\n",
    "    # Compute BLEU Score\n",
    "    bleu_score = calculate_bleu(example[\"completion\"], generated_code)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "    # Functional Testing (Check execution correctness)\n",
    "    execution_results.append(evaluate_execution(generated_code, example[\"completion\"]))\n",
    "\n",
    "# Compute Pass@3\n",
    "pass_at_k_score = pass_at_k(generated_outputs, [ex[\"completion\"] for ex in test_dataset], k=3)\n",
    "\n",
    "# Compute Average BLEU & Execution Accuracy\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "execution_accuracy = sum(execution_results) / len(execution_results)\n",
    "\n",
    "# --------------------------\n",
    "# 8. Print Final Results\n",
    "# --------------------------\n",
    "print(\"\\n--- Fine-Tuned CodeLlama Evaluation Results ---\")\n",
    "print(f\"Perplexity (Lower is better): {avg_perplexity:.3f}\")\n",
    "print(f\"BLEU Score (Higher is better): {avg_bleu:.3f}\")\n",
    "print(f\"Pass@3 (Higher is better): {pass_at_k_score:.3f}\")\n",
    "print(f\"Execution Accuracy (Higher is better): {execution_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe52c4e8-855d-4506-9292-d484ec7b2178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"codellama-7b-instruct-lora/checkpoint-2109\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")  # Base tokenizer\n",
    "tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"✅ Tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ded52071-e169-47a6-bcdb-ce48c267b82b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<QUESTION>what is dynamic programming</QUESTION>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"<QUESTION>what is dynamic programming</QUESTION>\", return_tensors=\"pt\", truncation=True, max_length=256).to(\"cpu\")\n",
    "    \n",
    "    # Generate CodeLlama Output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)  # ✅ Use max_new_tokens\n",
    "\n",
    "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147632d-e505-4d37-9334-aa0d5fc727c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c347fb1-c9d4-42fa-8dd3-f2e16951e264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
